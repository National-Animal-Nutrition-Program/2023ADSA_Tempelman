---
title: "St-Pierre 2001 Example"
subtitle: "National Animal Nutrition Program Meta-Analysis Workshop"
author: "Robert J. Tempelman"
affiliation: "Michigan State University"
date: "June 25, 2023"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,root.dir="C:/Users/Robert J. Tempelman/OneDrive - Michigan State University/Tempelman/Meta_analysis/StPierre")
# set default directory
```

```{r prep}
rm(list=ls())  # always a good idea to clean-up the directory
library(tidyverse,ggplot)
```

## Introduction

We'll revisit the simulated data provided by Dr. St_Pierre in the following paper

"*Invited Review: Integrating Quantitative Findings from Multiple Studies Using Mixed Model Methodology*" by N.R. St-Pierre
**J. Dairy Sci. 84:741-755** https://www.sciencedirect.com/science/article/pii/S0022030201745304 

The following is the original data as provided in the Appendix from St-Pierre (2001)... The first few observations are also printed with the head() function.  Let's double-check the data against the Appendix

```{r origdata}
urlfile="https://raw.githubusercontent.com/Tempelman/Meta_analysis/main/Dataregs2.csv"
Dataregs2<-read_csv(url(urlfile)) # Data provided in Appendix of St-Pierre (2001)
str(Dataregs2)
head(Dataregs2)
```

How about a scatterplot of the data?...well it should really look like what's given in Figure 2a of St-Pierre (2001).
..but it doesn't: 

```{r origplot}
ggplot(Dataregs2,aes(x=X,y=Y)) + geom_point() +
  scale_x_continuous(breaks=seq(0,12,2)) + scale_y_continuous(breaks=seq(-4,20,2))
```

Corrected data was kindly provided by Dr. White as forwarded from Dr. St-Pierre.  


```{r reviseddata}
urlfile2="https://raw.githubusercontent.com/Tempelman/Meta_analysis/main/Dataregscorrected.csv"
Dataregs1<-read_csv(url(urlfile2)) # Revised data
head(Dataregs1)
```
such that the revised corrected scatterplot looks as follows (and precisely what is provided in Figure 2A in St Pierre 2001):

```{r revplot}
ggplot(Dataregs1,aes(x=X,y=Y)) + geom_point() +
  scale_x_continuous(breaks=seq(0,12,2)) + scale_y_continuous(breaks=seq(-4,20,2)) + 
  ggtitle("Figure 2A in St-Pierre (2001)")
```

Ok...Dr. St-Pierre did not actually generate this data from a simple linear regression model: $y_i = \beta_0 + \beta_1{x_i} + e_i$  but  from the following *random coefficients* model:




$$y_{ij} = \beta_0 + s_i  + (\beta_1 + b_{i}) {x_{ij}} + e_{ij}; i=1,2,...,n_{study}; j=1,2,...,n_{i}$$

Note that *i* is used to index the study ($n_{study} = 20 in St-Pierre's example) whereas *ij* is used to index observation *j* within study *i*.

Here $e_{ij} \sim N(0,0.25)$  Furthermore,

$$\begin{pmatrix}
s_i  \\
b_{i}
\end{pmatrix}\sim N\left(\begin{pmatrix}
0 \\
0
\end{pmatrix},\begin{pmatrix}
4 & 0.5\sqrt{4}\sqrt{0.04} \\
0.5\sqrt{4}\sqrt{0.04} & 0.04
\end{pmatrix}\right)$$


In other words, the study-specific intercepts had a variance of 4 about a mean of 0 whereas the study-specific slopes had a variance of 0.04 about a mean of 1 such that there is an across-study correlation of 0.5 between intercepts and slopes.

Let's fit an overall regression model that ignores the study-specific heterogeneity and fit a simple linear regression model: $y_i = \beta_0 + \beta_1{x_i} + e_i$.  In other words, we'll generate the same results as **Figure 3** in St-Pierre (2001) who used SAS.


```{r Figure3}
overall_regression = lm(Y~X,data=Dataregs1)
summary(overall_regression)  
anova(overall_regression)
```
Wonderful!  We get the exact same estimates as provided by St-Pierre!!!  But they sure seem a lot different from the "truth"!  (intercept 0 slope 1).

THe following plot superimposes the line of best fit (blue) and intercept 0 slope 1 line (dashed) just as in Figure 4 of St-Pierre (2001)! 
```{r Figure4}
ggplot(Dataregs1, aes(x=X, y=Y)) + 
  geom_point()+
  geom_smooth(method=lm, se=FALSE) + 
  geom_abline(intercept =0, slope = 1,lty=2) +
  ggtitle ("Figure 4 in St-Pierre 2001")
```

Let's also reproduce the residual plot (Figure 5 in St-Pierre, 2001)

```{r Figure5}
plot(overall_regression,which=1)
title("Figure 5 in St Pierre (2001)",line=-2,adj=0.2)
```

## Accounting for heterogeneity between studies:  Fixed effects model

So let's account for the heterogeneity using the fixed effects model approach first presented by St-Pierre (2001) on **page 745**.
This is not really a great idea especially if the number of records per study is low and the distribution of Study effects is reasonably symmetric.
Also scope of inference is then limited to these specific studies only and not to future studies (implications for prediction intervals).
Mixed/random effects model analysis would be much better (broad scope inference).  See later 

The following reproduces much of the first page of **Figure 6** of St-Pierre (2001).  Based on corner (SAS) parameterization (zero out last level)

```{r Figure6page1}
Dataregs1$Study = factor(Dataregs1$Study)  # make sure Study is considered to be a factor (Study was numbered)
contrasts(Dataregs1$Study) <-contr.SAS(nlevels(Dataregs1$Study))  # use the SAS parameterization for comparison with StPierre
fixed_model = lm(Y~X+Study+X:Study,data=Dataregs1)   
summary(fixed_model)  # reproduce the estimates provided on first page of Figure 6
``` 

The following produces the least squares means for Study also provided at the **top of page 747** of St-Pierre (2001)

```{r Page 747top}
library(emmeans)
lsmeans_study= emmeans(fixed_model,"Study")
xgrid<-ref_grid(fixed_model,at=list(X=0))  # define the Study lsmeans at X = 0.
summary(xgrid)
```
The following is used to generate the study specific slopes provided at the **bottom of page 747** of St-Pierre (2001).

```{r Page 747bottom}
#Study specific slopes provided at bottom of page 747
library(multcomp)
# following reproduces the contrasts for the Study specific slopes in Equation [6] from St-Pierre
Slope_contrast = matrix(rep(0,21*40),nrow=21)
Slope_contrast[,2]=1
Slope_contrast[1,22:40]=1/20
Slope_contrast[2:20,22:40]=diag(19)
colnames(Slope_contrast) = names(coef(fixed_model))
rownames(Slope_contrast) = c("Overall Slope",paste0('Study',1:20))
print(Slope_contrast)  # these are provided in Equation [6] from St-Pierre
Slopes <- glht(fixed_model, linfct = Slope_contrast)
summary(Slopes)
```

The following prints the overall intercept reported near the **bottom of Figure 6 (page 1747)**

```{r Page 747overallint}
Overall_int_contrast = matrix(rep(0,1*40),nrow=1)
Overall_int_contrast[1,1] = 1
Overall_int_contrast[1,3:21]=0.05

colnames(Overall_int_contrast) = names(coef(fixed_model))
print(Overall_int_contrast)
Overall_int <- glht(fixed_model, linfct = Overall_int_contrast)
summary(Overall_int)
```
Finally....the ANOVA table as also provided at the **top of page 746** of St-Pierre (2001)

```{r ANOVAfixed}
library(car)
contrasts(Dataregs1$Study) <-contr.sum(nlevels(Dataregs1$Study)) # Type 3 ANOVA involving interactions with covariates only works well with contr.sum
fixed_model = lm(Y~X+Study+X:Study,data=Dataregs1)  # rerun same model again
summary(fixed_model)  # notice overall estimates of intercept and x match up with what is reported here.
anova(fixed_model)         # Anova Type = 1 (never look at those!)
Anova(fixed_model,type=3)
lsmeans_study= emmeans(fixed_model,"Study")  # doesn't change the least squares means for study though
```

## Accounting for heterogeneity between studies:  Mixed effects model

Treating Study-specific intercepts and slopes now as random effects
This is also known as a *random coefficients model*, otherwise known as a *random regression model* by your animal breeding colleagues.

The following reproduces much of **Figure 7**

```{r Figure7}
library(lme4)
mixed_model = lmer(Y~X+(1+X|Study),data=Dataregs1)
anova(mixed_model)
summary(mixed_model)

vc = VarCorr(mixed_model)
print(vc,comp=c("Variance","Std.Dev"))  # print the variance component estimates
coef(mixed_model)  # these don't look like the bottom of page 750 or top of page 751 because they need to be centered around zero.
```

The residual plot
```{r mixed_residplot}
 plot(mixed_model,which=1,main = "Residual plot for Mixed Effects Model")
```

**Bottom of Figure 7 from St-Pierre (2001):**

Study random effects for intercept and slope expressed as a difference from the mean

```{r Figure7bottom}
Study_specific_int = coef(mixed_model)$Study["(Intercept)"]-mean(coef(mixed_model)$Study["(Intercept)"][,1])
Study_specific_slope = coef(mixed_model)$Study["X"]-mean(coef(mixed_model)$Study["X"][,1])
data.frame(Study_specific_int,Study_specific_slope)
```


**Figure 10 from St-Pierre (2001)**
```{r Figure10}
Xmat = cbind(1,Dataregs1$X)
parmest = coef(summary(mixed_model))[,'Estimate']
y_adj=Xmat%*%parmest+resid(mixed_model)
data_adj = data.frame(Dataregs1$X,y_adj)
colnames(data_adj) = c("X","Y_adj") 

ggplot(data_adj,aes(x=X,y=Y_adj)) + geom_point() +
  scale_x_continuous(breaks=seq(0,12,2)) + scale_y_continuous(breaks=seq(-4,20,2))+ 
  geom_smooth(method=lm, se=FALSE) + 
  geom_abline(intercept =0, slope = 1,lty=2) +
  ggtitle("Figure 10 from St-Pierre 2001")
```

# Meta-analysis using study-specific sample least-squares estimates of slopes

let's conduct a separate regression analysis for each study:

first need to write a function for this.

```{r separate}
linear_model = function(df) {
  lm(Y~X,data=df)
}

library(broom)  # needed for tidy function below.
Dataregs1_analysis = Dataregs1 %>%
    group_by(Study) %>%
    nest() %>%
  mutate(model = map(data,linear_model)) %>%
  mutate(tidied=map(model,tidy)) %>% 
  mutate(nrec = as.numeric(map(data,nrow))) %>% 
  unnest(tidied) %>%
  mutate(term = ifelse(term=="(Intercept)","Intercept",term))%>%
  dplyr::select(-c(data,model))  

data.frame(Dataregs1_analysis)
```
We probably shouldn't include studies with only 2 records....standard errors are not defined!!
Remove studies 'E' and 'T'

Let's do the random coefficients mixed model analysis again on the original raw data but without those studies

```{r redomixed}
Dataregs1a = Dataregs1 %>%
   filter(!Study %in% c('E', 'T')) %>% 
   droplevels()
mixed_model = lmer(Y~X+(1+X|Study),data=Dataregs1a)
summary(mixed_model)
```
 
Meta-analyses software typically requires either determination of the weights (inverse squared standard errors) or sampling variances (squared standard errors) 
 
```{r addweights}
 Dataregs1_analysis = Dataregs1_analysis %>%
  na.omit() %>%
  mutate(weight = (1/std.error^2)) %>%
  mutate(sampvar = std.error^2)
  head(Dataregs1_analysis)
  
  # put the study specific intercepts and study specific slopes in separate files.
  Dataregs_intercepts = Dataregs1_analysis %>% filter(term=="Intercept")
  Dataregs_slopes = Dataregs1_analysis %>% filter(term=="X")
  head(Dataregs_intercepts)
  head(Dataregs_slopes)
 
```
# Common Effects Analysis

The following analysis is rather common and is typically the wrong analysis to use if there is indeed study to study heterogeneity.
Problem:: it often substantially understates uncertainty on the meta-estimates themselves
 
 
## Common Effects Analysis (1)


 
```{r commoneffects} 

# COMMON EFFECTS MODEL
cat("Overall estimated regression coefficient",sep='\n')
(weighted.mean(Dataregs_slopes$estimate,Dataregs_slopes$weight))
cat("Estimated Standard Error of Overall estimated regression coefficient",sep='\n')
(stderr = (sum(Dataregs_slopes$weight)^(-1/2)))
```
Relative to analysis of original data, point estimate seems fine but estimated standard error is understated

## Common Effects Analysis (2)
using mixed effects software (glmmTMB package).  
The trick is to hold residual variance constant to 1 and specify weights as inverse of sampling variances

for more details on how to fix variance components in linear mixed models (glmmTMB package) in r, refer to https://stackoverflow.com/questions/75569862/linear-mixed-model-with-fix-var-component-in-r 

It is really MUCH more easier to do this sort of thing in SAS.   See https://pubmed.ncbi.nlm.nih.gov/27111798/  and https://link.springer.com/article/10.1186/1471-2288-14-61 


```{r commoneffects2} 
library(glmmTMB)
CE_1 <- glmmTMB(estimate ~ 1 ,  
                weights = weight,
                family = gaussian,
                REML=TRUE,
                data = Dataregs_slopes,
                start = list(betad = log(1)),  # fix residual variance = 1
                map = list(betad = factor(NA))
)
summary(CE_1)
```
## Common Effects Analysis (3)
 using the R package metafor https://wviechtb.github.io/metafor/ 
 
```{r commoneffects3} 
library(metafor)
(res_EE = rma.uni(yi=estimate ,vi=sampvar,data=Dataregs_slopes,method="EE")) 
```
## Forest plot

great way to visualize undercertainty of individual studies along with meta-estimate and meta-CI

```{r forestcommon} 
forest(res_EE)
```

# Mixed model analysis

A mixed model analysis more appropriately accounts for between study heterogeneity in the treatment effects!

## Mixed model (1)
Random effects model using mixed models software.
The trick is to hold residual variance constant to 1

Compare to mixed model analysis on raw data.

```{r randomeffects1} 
RM_1 <- glmmTMB(estimate ~ 1 + (1|Study),   
                weights = weight,
                family = gaussian,
                REML=TRUE,
                data = Dataregs_slopes,
                start = list(betad = log(1)),  # fix residual variance = 1
                map = list(betad = factor(NA))
)
summary(RM_1)
```
## Mixed model (2)
Heterogeneous effects model using **metafor**.
Notice that **metafor** requires the specification of sampling variances, not weights (just inverses of each other)
```{r randomeffects2} 
(res_REML = rma.uni(yi=estimate,vi=sampvar,data=Dataregs_slopes,method="REML"))  # default
forest(res_REML)
```
Compare the width of the confidence interval for the mixed model (immediately above as the bottom interval of [0.97,1.20]) to that previously provided for the common effects model



```{r diagnostics} 
rstudent.rma.uni(res_REML)
### calculate influence diagnostics
inf <- influence(res_REML)
 
### plot the influence diagnostics
plot(inf)
dfbetas(res_REML)
```

## Likelihood ratio test to test for importance of heterogeneity
```{r LRT} 
anova(CE_1,RM_1)

# this works too:

anova(res_EE,res_REML)

```
# Analysis of intercepts

Could also do the same thing for intercepts.  Let's do the mixed model analysis there.

```{r random_intercepts} 
(res_int_REML = rma.uni(yi=estimate,vi=sampvar,data=Dataregs_intercepts,method="REML"))  # default
forest(res_int_REML)

```

# Suppose you didn't use the "trick" (just decided to try to estimate a residual variance)

```{r CE_forgottofix} 
CE_1forgot <- glmmTMB(estimate ~ 1 ,   
                weights = weight,
                family = gaussian,
                REML=TRUE,
                data = Dataregs_slopes
)
summary(CE_1forgot)

```

Wow, you would badly understate the uncertainty!!

The mixed effects model accounting for Study heterogeneity (adding (1|Study)) would not even run if you tried to estimate the residual variance (get an error message)


# Multivariate analysis of intercept and slope.

More statisticians advocate a **multivariate mixed model analysis** when inferring upon two or more "effect sizes" in a meta-analysis since they are likely to be correlated.  In fact, even if you're just interested in estimating slope (not intercept), it may benefit to conduct a multivariate analysis as we'll see in other examples.

I think think of two possible coding strategies for this.  Again the SAS options provided by Madden et al. (2016) (https://pubmed.ncbi.nlm.nih.gov/27111798/) might be more comprehensive.

## Multivariate mixed model (1)
using glmmTMB (mixed model analysis)

```{r mv1} 
RMM_1 <- glmmTMB(estimate ~ term + (term-1|Study),   
                weights = weight,
                family = gaussian,
                REML=TRUE,
                data = Dataregs1_analysis,
                start = list(betad = log(1)),  # fix residual variance = 1
                map = list(betad = factor(NA))
)
summary(RMM_1)
lsmeans_RMM_1= emmeans(RMM_1,"term")
summary(lsmeans_RMM_1)

```


## Multivariate mixed model (2)
using metafor  (forest plot might need some work for better formatted output.)
```{r mv2}
(res.mv <- rma.mv(estimate, sampvar, mods = ~ term -1, random = ~ term | Study, struct="UN", data=Dataregs1_analysis))

forest(res.mv)
```
